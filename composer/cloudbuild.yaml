steps:
  # Install the Python dependencies you need for your DAGs
  # Pin dependencies to the same versions used in your Composer instance by specifying a constraints file
  # https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html#constraints-files
  # Match your Airflow & Python version using the following templated link:
  # https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt
  # Run DAG unit tests 
  - name: apache/airflow:2.10.2-python3.11
    dir: composer
    script: |
      #!/usr/bin/env bash
      set -eo pipefail  # Enable exit-on-error mode
      pip install \
        --requirement tests/requirements.txt \
        --constraint https://raw.githubusercontent.com/apache/airflow/constraints-2.10.2/constraints-3.11.txt
      airflow standalone &
      airflow db check
      python3 -m pytest -s tests
  # Copy DAGs to the Composer /dags Cloud Storage folder only if unit tests pass
  - name: gcr.io/cloud-builders/gcloud
    script: |
      #!/usr/bin/env bash
      set -eo pipefail  # Enable exit-on-error mode
      # Get the DAGs folder URI for all Composer environments in the specified location and copy DAGs to them.
      gcloud composer environments list --locations=us-east4 --format="get(storageConfig.bucket)" | while read -r bucket; do
        if [[ -n "$bucket" ]]; then
          gcloud storage cp composer/dags/*.py gs://${bucket}/dags
        fi
      done
      gcloud composer environments list --locations=us-central1 --format="get(storageConfig.bucket)" | while read -r bucket; do
        if [[ -n "$bucket" ]]; then
          gcloud storage cp composer/dags/*.py gs://${bucket}/dags
        fi
      done
